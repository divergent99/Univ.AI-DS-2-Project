{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTFln2uXZmUD",
    "outputId": "9de35232-2982-4a60-bea6-6c8c11be798d"
   },
   "outputs": [],
   "source": [
    "# Additional required packages\n",
    "\n",
    "!pip install eli5\n",
    "!pip install geocoder\n",
    "!pip install holoviews\n",
    "!pip install hvplot\n",
    "!pip install geopy\n",
    "!pip install plotly\n",
    "!pip install panel\n",
    "!pip install statsmodels\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0sJ75kjZmUH",
    "outputId": "f83bd42c-501c-44a2-cf98-d004b15b2cd8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "# !pip install eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import colors\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import KFold\n",
    "from geopy.geocoders import Nominatim\n",
    "from itertools import product\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer, mean_absolute_error\n",
    "from collections import OrderedDict\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from geopy.geocoders import Nominatim\n",
    "import geocoder\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from lime import lime_tabular\n",
    "from matplotlib.pyplot import figure\n",
    "hv.extension('bokeh')\n",
    "pn.extension('tabulator')\n",
    "PALETTE = [\"#ff6f69\", \"#ffcc5c\", \"#88d8b0\", ]\n",
    "# from lazypredict.Supervised import LazyRegressor\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from prettytable import PrettyTable\n",
    "import eli5\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjkozzgGZmUI"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo5m7MJvZmUJ"
   },
   "outputs": [],
   "source": [
    "bangalore_df = pd.read_csv(\"Bangalore.csv\")\n",
    "hyderabad_df = pd.read_csv(\"Hyderabad.csv\")\n",
    "mumbai_df = pd.read_csv(\"Mumbai.csv\")\n",
    "chennai_df = pd.read_csv(\"Chennai.csv\")\n",
    "delhi_df = pd.read_csv(\"Delhi.csv\")\n",
    "kolkata_df = pd.read_csv(\"Kolkata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRIJjEggZmUJ",
    "outputId": "9a463b36-b680-438c-c5ac-26fe5937d470"
   },
   "outputs": [],
   "source": [
    "print(\"bangalore_df shape:\", bangalore_df.shape)\n",
    "print(\"hyderabad_df shape:\", hyderabad_df.shape)\n",
    "print(\"mumbai_df shape:\", mumbai_df.shape)\n",
    "print(\"chennai_df shape:\", chennai_df.shape)\n",
    "print(\"delhi_df shape:\", delhi_df.shape)\n",
    "print(\"kolkata_df shape:\", kolkata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNUHOl27ZmUK",
    "outputId": "1d741bbf-5c4a-4ad5-f307-ee2ab08733ba"
   },
   "outputs": [],
   "source": [
    "bangalore_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIWdcqoIZmUL"
   },
   "outputs": [],
   "source": [
    "bangalore_df.replace(9, np.nan, inplace=True)\n",
    "hyderabad_df.replace(9, np.nan, inplace=True)\n",
    "mumbai_df.replace(9, np.nan, inplace=True)\n",
    "chennai_df.replace(9, np.nan, inplace=True)\n",
    "delhi_df.replace(9, np.nan, inplace=True)\n",
    "kolkata_df.replace(9, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ItNvSRBZmUM"
   },
   "outputs": [],
   "source": [
    "bangalore_df = bangalore_df.dropna()\n",
    "hyderabad_df=hyderabad_df.dropna()\n",
    "mumbai_df=mumbai_df.dropna()\n",
    "chennai_df=chennai_df.dropna()\n",
    "delhi_df=delhi_df.dropna()\n",
    "kolkata_df=kolkata_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuF_gylsZmUN",
    "outputId": "48853ebc-7a54-4469-8076-15f76b0fc167"
   },
   "outputs": [],
   "source": [
    "print(\"bangalore_df shape:\", bangalore_df.shape)\n",
    "print(\"hyderabad_df shape:\", hyderabad_df.shape)\n",
    "print(\"mumbai_df shape:\", mumbai_df.shape)\n",
    "print(\"chennai_df shape:\", chennai_df.shape)\n",
    "print(\"delhi_df shape:\", delhi_df.shape)\n",
    "print(\"kolkata_df shape:\", kolkata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IH21e-kQZmUO"
   },
   "outputs": [],
   "source": [
    "bangalore_df['Address'] = bangalore_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Bangalore, India'))\n",
    "hyderabad_df['Address'] = hyderabad_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Hyderabad, India'))\n",
    "mumbai_df['Address'] = mumbai_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Mumbai, India'))\n",
    "chennai_df['Address'] = chennai_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Chennai, India'))\n",
    "delhi_df['Address'] = delhi_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Delhi, India'))\n",
    "kolkata_df['Address'] = kolkata_df['Location'].apply(lambda x: \"{}{}\".format(x, ', Kolkata, India'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TkADvcnZmUP"
   },
   "outputs": [],
   "source": [
    "bangalore_df['City'] = \"Bangalore\"\n",
    "hyderabad_df['City'] = \"Hyderabad\"\n",
    "mumbai_df['City'] = \"Mumbai\"\n",
    "chennai_df['City'] = \"Chennai\"\n",
    "delhi_df['City'] = \"Delhi\"\n",
    "kolkata_df['City'] = \"Kolkata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_HuSVi-ZmUP"
   },
   "outputs": [],
   "source": [
    "# frames = [bangalore_df, hyderabad_df, mumbai_df, chennai_df, delhi_df, kolkata_df]\n",
    "# merged_df = pd.concat(frames)\n",
    "# merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u34JGIfrZmUQ",
    "outputId": "a06a9079-9ab3-4ba4-b9e6-01385da95ac3"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(18, 18))\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.histplot(data = bangalore_df, x = \"Price\", kde = True, ax=axs[0, 0], bins='auto', log_scale=True)\n",
    "axs[0, 0].set_title('Bangalore Price Distribution', fontsize =16)\n",
    "axs[0, 0].set_xlabel('Price')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "xh, yh = axs[0,0].get_ylim()\n",
    "axs[0,0].vlines(bangalore_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "\n",
    "sns.histplot(data = hyderabad_df, x = \"Price\", kde = True, ax=axs[0, 1], bins='auto', log_scale=True)\n",
    "axs[0, 1].set_title('Hyderabad Price Distribution', fontsize =16)\n",
    "axs[0, 1].set_xlabel('Price Pressure')\n",
    "xh, yh = axs[0,1].get_ylim()\n",
    "axs[0,1].vlines(hyderabad_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "\n",
    "sns.histplot(data = mumbai_df, x = \"Price\", kde = True, ax=axs[1, 0], bins='auto', log_scale=True)\n",
    "axs[1, 0].set_title('Mumbai Price Distribution', fontsize =16)\n",
    "axs[1, 0].set_xlabel('Price')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "xh, yh = axs[1, 0].get_ylim()\n",
    "axs[1, 0].vlines(mumbai_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "\n",
    "sns.histplot(data = chennai_df, x = \"Price\", kde = True, ax=axs[1, 1], bins='auto', log_scale=True)\n",
    "axs[1, 1].set_title('Chennai Price Distribution', fontsize =16)\n",
    "axs[1, 1].set_xlabel('Price')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "xh, yh = axs[1, 1].get_ylim()\n",
    "axs[1, 1].vlines(chennai_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "\n",
    "sns.histplot(data = delhi_df, x = \"Price\", kde = True, ax=axs[2, 0], bins='auto', log_scale=True)\n",
    "axs[2, 0].set_title('Delhi Price Distribution', fontsize =16)\n",
    "axs[2, 0].set_xlabel('Price')\n",
    "axs[2, 0].set_ylabel('Frequency')\n",
    "xh, yh = axs[2,0].get_ylim()\n",
    "axs[2,0].vlines(delhi_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "\n",
    "sns.histplot(data = kolkata_df, x = \"Price\", kde = True, ax=axs[2, 1], bins='auto', log_scale=True)\n",
    "axs[2, 1].set_title('Kolkata Price Distribution', fontsize =16)\n",
    "axs[2, 1].set_xlabel('Price')\n",
    "axs[2, 1].set_ylabel('Frequency')\n",
    "xh, yh = axs[2, 1].get_ylim()\n",
    "axs[2,1].vlines(kolkata_df[\"Price\"].mean(), xh, yh, color='crimson', ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khnvx0y8ZmUR",
    "outputId": "bdd2f70c-2492-42c9-e0b6-405b2d326510"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3, 2, figsize=(18, 25))\n",
    "\n",
    "fig1 = bangalore_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Bangalore\")\n",
    "fig2 = hyderabad_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Hyderabad\")\n",
    "fig3 = mumbai_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Mumbai\") \n",
    "fig4 = chennai_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Chennai\")\n",
    "fig5 = delhi_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Delhi\")\n",
    "fig6 = kolkata_df[\"No. of Bedrooms\"].hvplot.hist(color = 'No. of Bedrooms', subplots=True, shared_axes=False, width=450, height=200, ylabel=\"No of Houses\", title = \"Kolkata\")\n",
    "\n",
    "(fig1 + fig2 + fig3 + fig4 + fig5 + fig6).cols(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFWbyzYSZmUR",
    "outputId": "e0c461ee-7ed2-4dd4-e394-28640a2fa27b"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "sns.boxplot(bangalore_df['Area'], ax=axs[0, 0])\n",
    "plt.title('Outliers In Area In Bangalore Dataset')\n",
    "\n",
    "sns.boxplot(hyderabad_df['Area'], ax=axs[0, 1])\n",
    "plt.title('Outliers In Area In Hyderabad Dataset')\n",
    "\n",
    "sns.boxplot(mumbai_df['Area'], ax=axs[1, 0])\n",
    "plt.title('Outliers In Area In Mumbai Dataset')\n",
    "\n",
    "sns.boxplot(chennai_df['Area'], ax=axs[1, 1])\n",
    "plt.title('Outliers In Area In Chennai Dataset')\n",
    "\n",
    "sns.boxplot(delhi_df['Area'], ax=axs[2, 0])\n",
    "plt.title('Outliers In Area In Delhi Dataset')\n",
    "\n",
    "sns.boxplot(kolkata_df['Area'], ax=axs[2, 1])\n",
    "plt.title('Outliers In Area In Kolkata Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSn3wi69ZmUS"
   },
   "outputs": [],
   "source": [
    "outlier_features = ['Area','No. of Bedrooms']\n",
    "for feature in outlier_features:\n",
    "    IQR = bangalore_df[feature].quantile(0.75) - bangalore_df[feature].quantile(0.25)\n",
    "    lower_boundary = bangalore_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = bangalore_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    bangalore_df.loc[bangalore_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    bangalore_df.loc[bangalore_df[feature]>=upper_boundary, feature] = upper_boundary\n",
    "    \n",
    "    IQR = hyderabad_df[feature].quantile(0.75) - hyderabad_df[feature].quantile(0.25)\n",
    "    lower_boundary = hyderabad_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = hyderabad_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    hyderabad_df.loc[hyderabad_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    hyderabad_df.loc[hyderabad_df[feature]>=upper_boundary, feature] = upper_boundary\n",
    "    \n",
    "    IQR = mumbai_df[feature].quantile(0.75) - mumbai_df[feature].quantile(0.25)\n",
    "    lower_boundary = mumbai_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = mumbai_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    mumbai_df.loc[mumbai_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    mumbai_df.loc[mumbai_df[feature]>=upper_boundary, feature] = upper_boundary\n",
    "    \n",
    "    IQR = chennai_df[feature].quantile(0.75) - chennai_df[feature].quantile(0.25)\n",
    "    lower_boundary = chennai_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = chennai_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    chennai_df.loc[chennai_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    chennai_df.loc[chennai_df[feature]>=upper_boundary, feature] = upper_boundary\n",
    "        \n",
    "    IQR = delhi_df[feature].quantile(0.75) - delhi_df[feature].quantile(0.25)\n",
    "    lower_boundary = delhi_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = delhi_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    delhi_df.loc[delhi_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    delhi_df.loc[delhi_df[feature]>=upper_boundary, feature] = upper_boundary\n",
    "\n",
    "    IQR = kolkata_df[feature].quantile(0.75) - kolkata_df[feature].quantile(0.25)\n",
    "    lower_boundary = kolkata_df[feature].quantile(0.25) - (IQR*1.5)\n",
    "    upper_boundary = kolkata_df[feature].quantile(0.75) + (IQR*1.5)\n",
    "    kolkata_df.loc[kolkata_df[feature]<=lower_boundary, feature] = lower_boundary\n",
    "    kolkata_df.loc[kolkata_df[feature]>=upper_boundary, feature] = upper_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7w3ie7jZmUT",
    "outputId": "66bcb0e1-53bb-4aaf-ca3f-b836d00a9f99"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "sns.boxplot(bangalore_df['Area'], ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Outliers In Area In Bangalore Dataset')\n",
    "\n",
    "sns.boxplot(hyderabad_df['Area'], ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Outliers In Area In Hyderabad Dataset')\n",
    "\n",
    "sns.boxplot(mumbai_df['Area'], ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Outliers In Area In Mumbai Dataset')\n",
    "\n",
    "sns.boxplot(chennai_df['Area'], ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Outliers In Area In Chennai Dataset')\n",
    "\n",
    "sns.boxplot(delhi_df['Area'], ax=axs[2, 0])\n",
    "axs[2, 0].set_title('Outliers In Area In Delhi Dataset')\n",
    "\n",
    "sns.boxplot(kolkata_df['Area'], ax=axs[2, 1])\n",
    "axs[2, 1].set_title('Outliers In Area In Kolkata Dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CtiIdr0ZmUU"
   },
   "outputs": [],
   "source": [
    "frames = [bangalore_df, hyderabad_df, mumbai_df, chennai_df, delhi_df, kolkata_df]\n",
    "merged_df = pd.concat(frames)\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu6ms_W6ZmUU"
   },
   "outputs": [],
   "source": [
    "def feature_generation(df):\n",
    "    loc_lat = {}\n",
    "    loc_lon = {}\n",
    "    a=0\n",
    "    loc = set(df[\"Address\"])\n",
    "    for i in loc: \n",
    "        location = geolocator.geocode(i)\n",
    "        try:\n",
    "            loc_lat[i] = location.latitude\n",
    "            loc_lon[i] = location.longitude\n",
    "            print(a)\n",
    "        except:\n",
    "            loc_lat[i] = np.nan\n",
    "            loc_lon[i] = np.nan\n",
    "        a=a+1\n",
    "    return loc_lat, loc_lon\n",
    "\n",
    "def getfinal(df):\n",
    "    df.replace(9,np.nan,inplace=True)\n",
    "    lat, lon= feature_generation(df)\n",
    "    df[\"Latitude\"] = [lat[i] for i in df[\"Address\"]]\n",
    "    df[\"Longitude\"] = [lon[i] for i in df[\"Address\"]]\n",
    "#     final_df = df.drop(columns = [\"Location\"])\n",
    "    df.dropna(inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taG-CPP-ZmUV"
   },
   "outputs": [],
   "source": [
    "# geolocator = Nominatim(user_agent=\"Merge\", timeout = 10)\n",
    "# merged_df1 = getfinal(merged_df)\n",
    "# merged_df1.to_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8Bi8nXMZmUW"
   },
   "outputs": [],
   "source": [
    "# merged_df1 = pd.read_csv(\"final.csv\")\n",
    "# merged_df1 = merged_df1.loc[:, ~merged_df1.columns.str.contains('^Unnamed')]\n",
    "# merged_df1.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7g7E5CtZmUW"
   },
   "outputs": [],
   "source": [
    "# ba = [\"No. of Bedrooms\", \"PowerBackup\", \"Gasconnection\", \"BED\", \"Refrigerator\", \"School\", \"Hospital\", \"Intercom\"]\n",
    "# assets = [i for i in merged_df1.columns if i not in ba+[\"Price\", \"Area\", \"Resale\", \"Latitude\", \"Longitude\", \"Location\", \"Address\", \"City\"]]\n",
    "\n",
    "num_Cols = ['Area', 'Latitude','Longitude']\n",
    "Cat_cols = [ 'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens', 'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility', 'ATM', 'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking', 'StaffQuarter', 'Cafeteria', 'MultipurposeRoom', 'Hospital', 'WashingMachine', 'Gasconnection', 'AC', 'Wifi',\"Children'splayarea\", 'LiftAvailable', 'BED', 'VaastuCompliant', 'Microwave', 'GolfCourse', 'TV', 'DiningTable', 'Sofa', 'Wardrobe', 'Refrigerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6Urk5_6ZmUX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pca_ba = PCA(n_components=(len(ba)))\n",
    "# pca_ba.fit(merged_df1[ba])\n",
    "# BAI_weights = pca_ba.explained_variance_ratio_\n",
    "\n",
    "# pca_AI = PCA(n_components=(len(assets)))\n",
    "# pca_AI.fit(merged_df1[assets])\n",
    "# AI_weights = pca_AI.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8POYA2CeZmUX"
   },
   "outputs": [],
   "source": [
    "# HQI=[]\n",
    "\n",
    "# for i in range(merged_df1.shape[0]):\n",
    "#     HQI.append(np.dot(BAI_weights, merged_df1[ba].iloc[i]) + np.dot(AI_weights, merged_df1[assets].iloc[i]))\n",
    "\n",
    "# merged_df1[\"HQI\"] = HQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBoX46AGZmUY"
   },
   "outputs": [],
   "source": [
    "# merged_df1.to_csv(\"final.csv\", index=False)\n",
    "merged_df1 = pd.read_csv(\"final.csv\")\n",
    "merged_df1 = merged_df1.loc[:, ~merged_df1.columns.str.contains('^Unnamed')]\n",
    "merged_df1.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQtlUQ1HZmUY",
    "outputId": "ece23813-7fcd-450e-a40b-47eb5ca2fa29"
   },
   "outputs": [],
   "source": [
    "merged_df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-KqAvVBZmUY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_list = ['bangalore_df', 'hyderabad_df', 'mumbai_df', 'chennai_df', 'delhi_df', 'kolkata_df']\n",
    "city_data_list = [bangalore_df, hyderabad_df, mumbai_df, chennai_df, delhi_df, kolkata_df]\n",
    "\n",
    "for df in city_data_list:\n",
    "    df['Price'] = pd.cut(df['Price'].astype('category'),\n",
    "                     bins=[0, 5000000, 10000000, 15000000, 20000000, 25000000, 30000000, 35000000, 40000000, 45000000, 50000000, 55000000, 60000000, np.inf],\n",
    "                     labels=['0-50', '50-100', '100-150', '150-200', '200-250', '250-300', '300-350', '350-400', '400-450', '450-500', '550-600', '650-700', '700+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM54o3mWZmUZ",
    "outputId": "ad64c64a-903b-422e-d8e2-265ddb181947"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, figsize=(18, 40))\n",
    "\n",
    "bangalore_df['Price'] = bangalore_df['Price'].astype('category')\n",
    "hyderabad_df['Price'] = hyderabad_df['Price'].astype('category')\n",
    "mumbai_df['Price'] = mumbai_df['Price'].astype('category')\n",
    "chennai_df['Price'] = chennai_df['Price'].astype('category')\n",
    "delhi_df['Price'] = delhi_df['Price'].astype('category')\n",
    "kolkata_df['Price'] = kolkata_df['Price'].astype('category')\n",
    "\n",
    "\n",
    "sns.countplot(x='Price', data=bangalore_df, ax=axs[0])\n",
    "axs[0].set_title('Bangalore Price Distribution', fontsize =16)\n",
    "axs[0].set_xlabel('Price')\n",
    "axs[0].set_ylabel('No of Houses')\n",
    "\n",
    "sns.countplot(x='Price', data=hyderabad_df, ax=axs[1])\n",
    "axs[1].set_title('Hyderabad Price Distribution', fontsize =16)\n",
    "axs[1].set_xlabel('Price')\n",
    "axs[1].set_ylabel('No of Houses')\n",
    "\n",
    "sns.countplot(x='Price', data=mumbai_df, ax=axs[2])\n",
    "axs[2].set_title('Mumbai Price Distribution', fontsize =16)\n",
    "axs[2].set_xlabel('Price')\n",
    "axs[2].set_ylabel('No of Houses')\n",
    "\n",
    "sns.countplot(x='Price', data=chennai_df, ax=axs[3])\n",
    "axs[3].set_title('Chennai Price Distribution', fontsize =16)\n",
    "axs[3].set_xlabel('Price')\n",
    "axs[3].set_ylabel('No of Houses')\n",
    "\n",
    "sns.countplot(x='Price', data=delhi_df, ax=axs[4])\n",
    "axs[4].set_title('Delhi Price Distribution', fontsize =16)\n",
    "axs[4].set_xlabel('Price')\n",
    "axs[4].set_ylabel('No of Houses')\n",
    "\n",
    "sns.countplot(x='Price', data=kolkata_df, ax=axs[5])\n",
    "axs[5].set_title('Kolkata Price Distribution', fontsize =16)\n",
    "axs[5].set_xlabel('Price')\n",
    "axs[5].set_ylabel('No of Houses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-895eUF4ZmUa",
    "outputId": "7035f94a-3e19-42a1-91af-da07f801691a"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(18, 30))\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.histplot(data = bangalore_df, x = \"Price\", kde = True, ax=axs[0, 0], bins='auto', hue = \"Resale\")\n",
    "axs[0, 0].set_title('Bangalore Price Distribution', fontsize =16)\n",
    "axs[0, 0].set_xlabel('Price', fontsize=15)\n",
    "axs[0, 0].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "sns.histplot(data = hyderabad_df, x = \"Price\", kde = True, ax=axs[0, 1], bins='auto', hue = \"Resale\")\n",
    "axs[0, 1].set_title('Hyderabad Price Distribution', fontsize =16)\n",
    "axs[0, 1].set_xlabel('Price', fontsize=15)\n",
    "axs[0, 1].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "sns.histplot(data = mumbai_df, x = \"Price\", kde = True, ax=axs[1, 0], bins='auto', hue = \"Resale\")\n",
    "axs[1, 0].set_title('Mumbai Price Distribution', fontsize =16)\n",
    "axs[1, 0].set_xlabel('Price', fontsize=15)\n",
    "axs[1, 0].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "sns.histplot(data = chennai_df, x = \"Price\", kde = True, ax=axs[1, 1], bins='auto', hue = \"Resale\")\n",
    "axs[1, 1].set_title('Chennai Price Distribution', fontsize =16)\n",
    "axs[1, 1].set_xlabel('Price', fontsize=15)\n",
    "axs[1, 1].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "sns.histplot(data = delhi_df, x = \"Price\", kde = True, ax=axs[2, 0], bins='auto', hue = \"Resale\")\n",
    "axs[2, 0].set_title('Delhi Price Distribution', fontsize =16)\n",
    "axs[2, 0].set_xlabel('Price', fontsize=15)\n",
    "axs[2, 0].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[2, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "sns.histplot(data = kolkata_df, x = \"Price\", kde = True, ax=axs[2, 1], bins='auto', hue = \"Resale\")\n",
    "axs[2, 1].set_title('Kolkata Price Distribution', fontsize =16)\n",
    "axs[2, 1].set_xlabel('Price', fontsize=15)\n",
    "axs[2, 1].set_ylabel('No of Houses', fontsize=15)\n",
    "axs[2, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAEjNGezZmUa"
   },
   "outputs": [],
   "source": [
    "idf = merged_df1.interactive()\n",
    "\n",
    "# Define Panel widgets\n",
    "No_of_Bedrooms = pn.widgets.IntSlider(name='No. of Bedrooms', start=1, end=8, step=1)\n",
    "Resale = pn.widgets.IntSlider(name='Resale', start=0, end=1, step=1)\n",
    "city = pn.widgets.ToggleGroup(\n",
    "    name='City',\n",
    "    options=['Bangalore', 'Chennai', 'Delhi', 'Hyderabad', 'Kolkata', 'Mumbai'], \n",
    "    value=['Bangalore', 'Chennai', 'Delhi', 'Hyderabad', 'Kolkata', 'Mumbai'],\n",
    "    button_type='success')\n",
    "yaxis = pn.widgets.RadioButtonGroup(\n",
    "    name='Y axis', \n",
    "    options=['Price', \"HQI\"],\n",
    "    button_type='success'\n",
    ")\n",
    "\n",
    "# Combine pipeline and widgets\n",
    "ipipeline = (\n",
    "    idf[\n",
    "        (idf['No. of Bedrooms'] == No_of_Bedrooms) & \n",
    "        (idf.City.isin(city)) &\n",
    "        (idf.Resale == Resale)\n",
    "    ]\n",
    "    .groupby([\"Location\", \"Area\", \"City\"])[yaxis].mean()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .sort_values(by='Area')  \n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9o7INQSZmUb"
   },
   "outputs": [],
   "source": [
    "iplot = ipipeline.hvplot(x=\"Area\", y=yaxis, color=\"City\", kind='scatter')\n",
    "itable = ipipeline.pipe(pn.widgets.Tabulator, pagination = 'remote', page_size = 10, sizing_mode='stretch_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45N_-nAXZmUb",
    "outputId": "9ffc7c0a-a69d-487d-83f1-cb23cf5c9c2b"
   },
   "outputs": [],
   "source": [
    "iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6cLpTdNZmUc",
    "outputId": "3a2152df-b88a-4b09-cd82-b2387d7409c5"
   },
   "outputs": [],
   "source": [
    "itable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yntrG0KZmUc",
    "outputId": "83681fa4-bfeb-459f-e0b5-2eee4308cf32"
   },
   "outputs": [],
   "source": [
    "merged_df1.hvplot.points('Longitude', 'Latitude', geo=True, color='red', alpha=0.2, tiles='EsriNatGeo',ylim=(0, 40), xlim=(60, 100), width=950, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RKA3jy4ZmUd",
    "outputId": "045cc589-abcd-4444-de75-2ea41b767888"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,15))\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "sns.heatmap(merged_df1.corr().abs(), ax = ax, cmap=\"YlGnBu\", vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhTZby9WZmUd"
   },
   "outputs": [],
   "source": [
    "merged_df['Area'] = np.log(merged_df['Area'])\n",
    "merged_df['Price'] = np.log(merged_df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOfoueziZmUd",
    "outputId": "ac928668-9d35-450a-f728-1386e7360b35"
   },
   "outputs": [],
   "source": [
    "merged_df.hvplot.bivariate(x='Area', y='Price', width=600, height=500, cmap='BrBG', title=\"Bivariate Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSc1h1XlZmUe",
    "outputId": "766073f4-1dee-4905-f3c6-102ef81229c3"
   },
   "outputs": [],
   "source": [
    "merged_df.hvplot.hexbin(x='Area', y='Price', width=600, height=500, logz=True, color = PALETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJCgSG5JZmUe",
    "outputId": "13490693-0f4f-4e09-d038-bf22640f8269"
   },
   "outputs": [],
   "source": [
    "merged_df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI8ab9ubZmUf",
    "outputId": "254bf67d-990a-4810-9f6c-1b03b9a77a9b"
   },
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "Standard_merged_df = merged_df1.copy()\n",
    "Standard_merged_df[[\"Area\", \"HQI\", \"Latitude\", \"Longitude\"]] = scalar.fit_transform(Standard_merged_df[[\"Area\", \"HQI\", \"Latitude\", \"Longitude\"]])\n",
    "Standard_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR6pE6wJZmUf"
   },
   "outputs": [],
   "source": [
    "X = merged_df1.drop([\"Price\", \"Location\", \"City\", \"Address\"], axis = 1)\n",
    "y = merged_df1[[\"Price\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBeEogy0ZmUf"
   },
   "outputs": [],
   "source": [
    "X_std = Standard_merged_df.drop([\"Price\", \"Location\", \"City\", \"Address\"], axis = 1)\n",
    "y_std = Standard_merged_df[[\"Price\"]]\n",
    "X_train_std, X_test_std, y_train_std, y_test_std = train_test_split(X_std, y_std, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQgZYFt_ZmUf",
    "outputId": "637d488c-b4f5-433b-cd01-5956e961cfce"
   },
   "outputs": [],
   "source": [
    "merged_df_Location = merged_df1[[\"Location\"]]\n",
    "merged_df_Location.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPkJA_4WZmUg"
   },
   "source": [
    "## AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgsT-m8-ZmUi"
   },
   "outputs": [],
   "source": [
    "best_param_dict={}\n",
    "acc={}\n",
    "\n",
    "def R2(y_true,y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eElndSK0ZmUi"
   },
   "outputs": [],
   "source": [
    "# pipe_adbr = Pipeline([('ADBR', AdaBoostRegressor())])\n",
    "# adbr_params = {'ADBR__n_estimators':[100, 200, 300, 500, 600, 700, 1000],\n",
    "#               'ADBR__learning_rate':np.logspace(-3,3,7),\n",
    "#               'ADBR__random_state': [42]\n",
    "#               }\n",
    "\n",
    "# adbr_model = GridSearchCV(pipe_adbr, adbr_params, n_jobs=-1, cv=10, scoring = 'r2')\n",
    "# adbr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-vC0NTiZmUj"
   },
   "outputs": [],
   "source": [
    "adbr_params = {'n_estimators':[100, 200, 300, 500, 600, 700, 1000], \n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "adbr_score = {}\n",
    "\n",
    "for n, l in product(*adbr_params.values()):\n",
    "    params = (n, l)\n",
    "    adbr = AdaBoostRegressor(random_state = 42, \n",
    "                             n_estimators = n, \n",
    "                             learning_rate = l)\n",
    "    \n",
    "    adbr.fit(X_train, y_train)\n",
    "    y_pred = adbr.predict(X_test)\n",
    "    adbr_score[params] = r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6Er23DbZmUj",
    "outputId": "bb579c52-a07e-4902-a890-9280be627f79"
   },
   "outputs": [],
   "source": [
    "best_param_adbr = max(adbr_score, key = adbr_score.get)\n",
    "\n",
    "best_param_dict[\"AdaBoost Regressor\"] = \"n_estimators: {}, learning_rate: {}\".format(best_param_adbr[0], \n",
    "                                                                                        best_param_adbr[1])\n",
    "\n",
    "print(\"Best n_estimator: {}\".format(best_param_adbr[0]))\n",
    "print(\"Best learning_rate: {}\".format(best_param_adbr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqtdy4MeZmUk",
    "outputId": "7abc4b43-8148-4f29-a2b0-416d00027fe1"
   },
   "outputs": [],
   "source": [
    "new_adbr = AdaBoostRegressor(random_state = 42, \n",
    "                             n_estimators = best_param_adbr[0], \n",
    "                             learning_rate = best_param_adbr[1])\n",
    "\n",
    "new_adbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MZSc6MLZmUk",
    "outputId": "b4f36707-497a-4360-b843-a309accd26d5"
   },
   "outputs": [],
   "source": [
    "y_pred = new_adbr.predict(X_test)\n",
    "residuals = np.array(y_test[\"Price\"]) - y_pred\n",
    "acc[\"AdaBoost Regressor\"] = R2(y_test, y_pred)*100\n",
    "print(\"Best Test Score:\", acc[\"AdaBoost Regressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weh_A9FiZmUk"
   },
   "outputs": [],
   "source": [
    "importances = new_adbr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LCLL8QaZmUl",
    "outputId": "672c18e0-ee92-47f7-adbc-e7b885723586"
   },
   "outputs": [],
   "source": [
    "dtf_importances = pd.DataFrame({\"IMPORTANCE\":importances, \n",
    "                                \"VARIABLE\":X_train.columns}).sort_values(\"IMPORTANCE\", ascending=False)\n",
    "dtf_importances['cumsum'] = dtf_importances['IMPORTANCE'].cumsum(axis=0)\n",
    "dtf_importances = dtf_importances.set_index(\"VARIABLE\")\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(25, 10))\n",
    "fig.suptitle(\"Features Importance\", fontsize=20)\n",
    "ax[0].title.set_text('variables')\n",
    "dtf_importances[[\"IMPORTANCE\"]].sort_values(by=\"IMPORTANCE\").plot(\n",
    "                kind=\"barh\", legend=False, ax=ax[0]).grid(axis=\"x\")\n",
    "ax[0].set(ylabel=\"\")\n",
    "ax[1].title.set_text('cumulative')\n",
    "dtf_importances[[\"cumsum\"]].plot(kind=\"line\", linewidth=4, \n",
    "                                 legend=False, ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", xticks=np.arange(len(dtf_importances)), \n",
    "          xticklabels=dtf_importances.index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4LZi9fXZmUm",
    "outputId": "9b8616c5-9e66-4b0c-91b9-534942ece8bb"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=('True vs Predicted',  'Residuals vs Predicted'))\n",
    "\n",
    "# Scatter plot True vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=np.array(y_test[\"Price\"]), \n",
    "                mode = 'markers', \n",
    "                name = 'True vs Predicted'\n",
    "                ),\n",
    "    row=1, \n",
    "    col=1\n",
    "            )\n",
    "\n",
    "# Scatter plot Residuals vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=residuals, \n",
    "                mode = 'markers', \n",
    "                name = 'Residuals vs Predicted'\n",
    "              ),\n",
    "    row=1, \n",
    "    col=2\n",
    "            )\n",
    "\n",
    "# Setting theme, margin, and annotation in layout\n",
    "\n",
    "fig.update_layout(\n",
    "                    template=\"plotly_dark\", \n",
    "                    title=\"AdaBoost Regressor Best Results Plot\", \n",
    "                    xaxis_title=\"Predicted\", \n",
    "                    yaxis_title=\"True\",\n",
    "                    xaxis2_title=\"Predicted\", \n",
    "                    yaxis2_title=\"Residuals\",\n",
    "                    showlegend=False\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZYo_9WOZmUn"
   },
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(training_data= X_train.values, \n",
    "                                              feature_names= X_train.columns, \n",
    "                                              class_names = \"Price\", \n",
    "                                              mode=\"regression\")\n",
    "\n",
    "explained = explainer.explain_instance(X_test.values[2], new_adbr.predict, num_features=len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtMSkENXZmUn",
    "outputId": "84aa370e-e3ac-4e03-c36f-82840aaff4f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(12, 25))\n",
    "\n",
    "explained.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDVVUkBpZmUo",
    "outputId": "fede7f4a-84d2-4c5f-882f-9d51390e4aa8"
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(new_adbr, X_train, y_train, n_repeats = 30, random_state = 42, n_jobs = -1)\n",
    "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "x = result.importances[perm_sorted_idx].mean(axis=1).T\n",
    "y = X_train.columns[perm_sorted_idx]\n",
    "x_error = result.importances_std[perm_sorted_idx]\n",
    "sns.barplot(x=x, y=y, ax=ax, ci='sd')\n",
    "ax.errorbar(x, y, xerr = x_error, fmt='.', ecolor = 'black', color='black')\n",
    "ax.set_title(\"Permutation Importances (Train set)\", fontsize =16)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-J6Dia2ZmUo"
   },
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6E2UTV4SZmUp"
   },
   "outputs": [],
   "source": [
    "# pipe_rf = Pipeline([('RF',RandomForestRegressor(random_state=42))])\n",
    "\n",
    "# rf_params = OrderedDict(RF__n_estimators = [10, 30, 50, 100, 150], \n",
    "#                          RF__random_state = [42], \n",
    "#                          RF__max_features = (range(2, 10)), \n",
    "#                          RF__max_depth=(range(8,20)),\n",
    "#                          RF__oob_score = [True],\n",
    "#                          RF__warm_start = [True], \n",
    "#                         )\n",
    "\n",
    "# rf_model = GridSearchCV(pipe_rf, rf_params, n_jobs=-1, cv=10, scoring = \"r2\")\n",
    "# rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnRsHFrAZmUq"
   },
   "outputs": [],
   "source": [
    "param_dict = OrderedDict(n_estimators = [10,30, 50, 100, 150], \n",
    "                         max_features = (range(10, 42)), \n",
    "                         max_depth=(range(8,20)))\n",
    "param_dict.values()\n",
    "\n",
    "oob_results = {}\n",
    "\n",
    "for n, f, d in product(*param_dict.values()):\n",
    "    params = (n, f, d)\n",
    "    rf = RandomForestRegressor(oob_score = True, \n",
    "                                n_estimators = n, max_features = f, max_depth = d, \n",
    "                                random_state = 42, warm_start = False, n_jobs = -1)\n",
    "    rf.fit(X_train, y_train)\n",
    "#     y_pred = rf.predict(X_train)\n",
    "#     oob_results[params] = r2_score(y_pred, y_train)\n",
    "    oob_results[params] = rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgj9iqaRZmUq",
    "outputId": "c48dd4b1-793d-4be6-ae75-23bc4038f8c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_param_rf = max(oob_results, key = oob_results.get)\n",
    "\n",
    "best_param_dict[\"Random Forest Regressor\"] = \"n_estimators: {}, max_features: {}, max_depth: {}\".format(best_param_rf[0], \n",
    "                                                                                        best_param_rf[1],\n",
    "                                                                                        best_param_rf[2])\n",
    "\n",
    "print(\"Best n_estimator: {}\".format(best_param_rf[0]))\n",
    "print(\"Best max_features: {}\".format(best_param_rf[1]))\n",
    "print(\"Best max_depth: {}\".format(best_param_rf[2]))\n",
    "print(\"Best Training Score:\", max(oob_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fPyMuVhZmUr",
    "outputId": "f8028c58-220b-4783-b196-cf36c4ec6e55",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_rf = RandomForestRegressor(oob_score = True, \n",
    "                               n_estimators = best_param_rf[0], \n",
    "                               max_features = best_param_rf[1], \n",
    "                               max_depth = best_param_rf[2], \n",
    "                               random_state = 42, \n",
    "                               warm_start = False, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "new_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzcI3OYtZmUr",
    "outputId": "43bc0c8b-98c5-47e3-b290-bfb3318796b5"
   },
   "outputs": [],
   "source": [
    "y_pred = new_rf.predict(X_test)\n",
    "residuals = np.array(y_test[\"Price\"]) - y_pred\n",
    "acc[\"Random Forest Regressor\"] = R2(y_test, y_pred)*100\n",
    "print(\"Best Test Score:\", acc[\"Random Forest Regressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlAJoef4ZmUs"
   },
   "outputs": [],
   "source": [
    "importances = new_rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vx8cQwEZmUt",
    "outputId": "082b9e0d-414d-47aa-8d22-c0c194f21230"
   },
   "outputs": [],
   "source": [
    "dtf_importances = pd.DataFrame({\"IMPORTANCE\":importances, \n",
    "                                \"VARIABLE\":X_train.columns}).sort_values(\"IMPORTANCE\", ascending=False)\n",
    "dtf_importances['cumsum'] = dtf_importances['IMPORTANCE'].cumsum(axis=0)\n",
    "dtf_importances = dtf_importances.set_index(\"VARIABLE\")\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(25, 10))\n",
    "fig.suptitle(\"Features Importance\", fontsize=20)\n",
    "ax[0].title.set_text('variables')\n",
    "dtf_importances[[\"IMPORTANCE\"]].sort_values(by=\"IMPORTANCE\").plot(\n",
    "                kind=\"barh\", legend=False, ax=ax[0]).grid(axis=\"x\")\n",
    "ax[0].set(ylabel=\"\")\n",
    "ax[1].title.set_text('cumulative')\n",
    "dtf_importances[[\"cumsum\"]].plot(kind=\"line\", linewidth=4, \n",
    "                                 legend=False, ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", xticks=np.arange(len(dtf_importances)), \n",
    "          xticklabels=dtf_importances.index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h1GbH_uZmUt",
    "outputId": "4b42f266-b1ea-483d-cd6f-9388851e2f79"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=('True vs Predicted',  'Residuals vs Predicted'))\n",
    "\n",
    "# Scatter plot True vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=np.array(y_test[\"Price\"]), \n",
    "                mode = 'markers', \n",
    "                name = 'True vs Predicted'\n",
    "                ),\n",
    "    row=1, \n",
    "    col=1\n",
    "            )\n",
    "\n",
    "# Scatter plot Residuals vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=residuals, \n",
    "                mode = 'markers', \n",
    "                name = 'Residuals vs Predicted'\n",
    "              ),\n",
    "    row=1, \n",
    "    col=2\n",
    "            )\n",
    "\n",
    "# Setting theme, margin, and annotation in layout\n",
    "\n",
    "fig.update_layout(\n",
    "                    template=\"plotly_dark\", \n",
    "                    title=\"Random Forest Regressor Best Results Plot\", \n",
    "                    xaxis_title=\"Predicted\", \n",
    "                    yaxis_title=\"True\",\n",
    "                    xaxis2_title=\"Predicted\", \n",
    "                    yaxis2_title=\"Residuals\",\n",
    "                    showlegend=False\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJUlemBzZmUu"
   },
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(training_data= X_train.values, \n",
    "                                              feature_names= X_train.columns, \n",
    "                                              class_names = \"Price\", \n",
    "                                              mode=\"regression\")\n",
    "\n",
    "explained = explainer.explain_instance(X_test.values[2], new_rf.predict, num_features=len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwPvvZzGZmUu",
    "outputId": "c41abb6c-485f-469b-e7ea-69d70041a77e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explained.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enscnXqlZmUv",
    "outputId": "932ca7c6-8505-4a49-f4dc-2aa1a268dd02"
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(new_rf, X_train, y_train, n_repeats=30, random_state = 42, n_jobs=-1)\n",
    "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "x = result.importances[perm_sorted_idx].mean(axis=1).T\n",
    "y = X_train.columns[perm_sorted_idx]\n",
    "x_error = result.importances_std[perm_sorted_idx]\n",
    "sns.barplot(x=x, y=y, ax=ax, ci='sd')\n",
    "ax.errorbar(x, y, xerr = x_error, fmt='.', ecolor = 'black', color='black')\n",
    "ax.set_title(\"Permutation Importances (Train set)\", fontsize =16)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naUuSHZfZmUv"
   },
   "source": [
    "## Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4m3oks9ZmUv"
   },
   "outputs": [],
   "source": [
    "# pipe_br = Pipeline([('BR', BaggingRegressor(random_state=42))])\n",
    "\n",
    "# br_params = {'BR__n_estimators': range(30, 50), \n",
    "#          'BR__base_estimator__random_state': [42], \n",
    "#          'BR__base_estimator': [DecisionTreeRegressor()]\n",
    "#          }\n",
    "\n",
    "# br_model = GridSearchCV(pipe_br, br_params, n_jobs=-1, cv=10, scoring = 'r2')\n",
    "\n",
    "# br_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqMaNiPrZmUw"
   },
   "outputs": [],
   "source": [
    "param_dict = OrderedDict(n_estimators = [5, 10, 20], \n",
    "                         max_features = (range(10, 42)), \n",
    "                         max_samples = range(90, 100))\n",
    "param_dict.values()\n",
    "\n",
    "oob_results = {}\n",
    "\n",
    "for n, f, s in product(*param_dict.values()):\n",
    "    params = (n, f, s)\n",
    "    br = BaggingRegressor(DecisionTreeRegressor(), \n",
    "                          oob_score = True, \n",
    "                          n_estimators = n, \n",
    "                          max_features = f, \n",
    "                          max_samples = s, \n",
    "                          random_state = 42, \n",
    "                          warm_start = False, \n",
    "                          n_jobs = -1)\n",
    "    \n",
    "    br.fit(X_train, y_train)\n",
    "#     y_pred = br.predict(X_test)\n",
    "#     oob_results[params] = r2_score(y_pred, y_test)\n",
    "    oob_results[params] = rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbnMkFaYZmUw",
    "outputId": "44c06da9-cea7-46ee-ff85-33a2f3141de6"
   },
   "outputs": [],
   "source": [
    "best_param = max(oob_results, key = oob_results.get)\n",
    "best_param_dict[\"Bagging Regressor\"] = \"n_estimators: {}, max_features: {}, max_samples: {}\".format(best_param[0], \n",
    "                                                                                        best_param[1],\n",
    "                                                                                        best_param[2])\n",
    "print(\"Best n_estimator: {}\".format(best_param[0]))\n",
    "print(\"Best max_features: {}\".format(best_param[1]))\n",
    "print(\"Best max_samples: {}\".format(best_param[2]))\n",
    "print(\"Best Training Score:\", max(oob_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGRJAHsAZmUw",
    "outputId": "cffed9fa-14a1-4d7d-ef94-70d13626de06"
   },
   "outputs": [],
   "source": [
    "new_br = BaggingRegressor(DecisionTreeRegressor(), \n",
    "                          oob_score=True, \n",
    "                          n_estimators = best_param[0], \n",
    "                          max_features = best_param[1], \n",
    "                          max_samples = best_param[2], \n",
    "                          random_state = 42, \n",
    "                          warm_start = False, \n",
    "                          n_jobs = -1)\n",
    "\n",
    "new_br.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jEHFWgyZmUx",
    "outputId": "556899a0-d234-44b6-bf87-569d3156c88a"
   },
   "outputs": [],
   "source": [
    "y_pred = new_br.predict(X_test)\n",
    "residuals = np.array(y_test[\"Price\"]) - y_pred\n",
    "acc[\"Bagging Regressor\"] = R2(y_test, y_pred)*100\n",
    "print(\"Best Test Score:\", acc[\"Bagging Regressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScgKY2ETZmUx",
    "outputId": "9bac69a1-8371-47a9-b6fd-601b742c8dab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(new_br, X_train, y_train, n_repeats=30, random_state = 42, n_jobs=-1)\n",
    "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "x = result.importances[perm_sorted_idx].mean(axis=1).T\n",
    "y = X_train.columns[perm_sorted_idx]\n",
    "x_error = result.importances_std[perm_sorted_idx]\n",
    "sns.barplot(x=x, y=y, ax=ax, ci='sd')\n",
    "ax.errorbar(x, y, xerr = x_error, fmt='.', ecolor = 'black', color='black')\n",
    "ax.set_title(\"Permutation Importances (Train set) for Bagging Regressor\", fontsize =16)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_oGjt7nZmUy",
    "outputId": "6fd77581-c5ea-4914-f8d8-7888eedd5a97"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=('True vs Predicted',  'Residuals vs Predicted'))\n",
    "\n",
    "# Scatter plot True vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=np.array(y_test[\"Price\"]), \n",
    "                mode = 'markers', \n",
    "                name = 'True vs Predicted'\n",
    "                ),\n",
    "    row=1, \n",
    "    col=1\n",
    "            )\n",
    "\n",
    "# Scatter plot Residuals vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=residuals, \n",
    "                mode = 'markers', \n",
    "                name = 'Residuals vs Predicted'\n",
    "              ),\n",
    "    row=1, \n",
    "    col=2\n",
    "            )\n",
    "\n",
    "# Setting theme, margin, and annotation in layout\n",
    "\n",
    "fig.update_layout(\n",
    "                    template=\"plotly_dark\", \n",
    "                    title=\"Bagging Regressor Best Results Plot\", \n",
    "                    xaxis_title=\"Predicted\", \n",
    "                    yaxis_title=\"True\",\n",
    "                    xaxis2_title=\"Predicted\", \n",
    "                    yaxis2_title=\"Residuals\",\n",
    "                    showlegend=False\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9G8o8h6ZmUy"
   },
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(training_data= X_train.values, \n",
    "                                              feature_names= X_train.columns, \n",
    "                                              class_names = \"Price\", \n",
    "                                              mode=\"regression\")\n",
    "\n",
    "explained = explainer.explain_instance(X_test.values[2], new_br.predict, num_features=len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HI1MS0iYZmUz",
    "outputId": "3f326f63-d859-46e3-c745-5eeb561994a2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explained.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7a8vFvcZmU0"
   },
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Csaf1wsTZmU0"
   },
   "outputs": [],
   "source": [
    "# pipe_gbr = Pipeline([('GBR', GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# gbr_params = {'GBR__n_estimators':[100, 500,1000, 5000],\n",
    "#               'GBR__learning_rate':np.logspace(-3,3,7),\n",
    "#               'GBR__random_state': [42]\n",
    "#               }\n",
    "\n",
    "# gbr_model = GridSearchCV(pipe_gb, gb_params, cv=10, scoring = 'r2')\n",
    "\n",
    "# gbr_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLYgHe7IZmU1"
   },
   "outputs": [],
   "source": [
    "# print(gbr_model.best_params_)\n",
    "# print(gbr_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFYtbSVqZmU1"
   },
   "outputs": [],
   "source": [
    "gbr_params = {'n_estimators':[100, 200, 300, 500, 1000, 2000, 5000], \n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "gbr_score = {}\n",
    "\n",
    "for n, l in product(*gbr_params.values()):\n",
    "    params = (n, l)\n",
    "    gbr = GradientBoostingRegressor(random_state = 42, n_estimators = n, learning_rate = l)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_train)\n",
    "    gbr_score[params] = r2_score(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0WIXrVaZmU1",
    "outputId": "9a5caaa5-bc56-40b4-ecb0-6efea84877be"
   },
   "outputs": [],
   "source": [
    "best_param_gbr = max(gbr_score, key = gbr_score.get)\n",
    "\n",
    "best_param_dict[\"Gradient Boosting Regressor\"] = \"n_estimators: {}, learning_rate: {}\".format(best_param_gbr[0], \n",
    "                                                                                        best_param_gbr[1])\n",
    "\n",
    "print(\"Best n_estimator: {}\".format(best_param_gbr[0]))\n",
    "print(\"Best learning_rate: {}\".format(best_param_gbr[1]))\n",
    "print(\"Best Training Score:\", max(gbr_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kywm6a3DZmU2",
    "outputId": "92926cd0-760e-4075-9265-b299f6ca1bbb"
   },
   "outputs": [],
   "source": [
    "new_gbr = GradientBoostingRegressor(random_state = 42, \n",
    "                                    n_estimators = best_param_gbr[0], \n",
    "                                    learning_rate = best_param_gbr[1])\n",
    "\n",
    "new_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-RwQF3dZmU3",
    "outputId": "c5baf05d-aa27-4f07-ea8a-1b1af49f3f7d"
   },
   "outputs": [],
   "source": [
    "y_pred = new_gbr.predict(X_test)\n",
    "residuals = np.array(y_test[\"Price\"]) - y_pred\n",
    "acc[\"Gradient Boosting Regressor\"] = R2(y_test, y_pred)*100\n",
    "print(\"Best Test Score:\", acc[\"Gradient Boosting Regressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UVdbbUAZmU3"
   },
   "outputs": [],
   "source": [
    "importances = new_gbr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEdqUxfDZmU4",
    "outputId": "4270164d-00f4-4cbc-f39a-f5c416de6d38"
   },
   "outputs": [],
   "source": [
    "dtf_importances = pd.DataFrame({\"IMPORTANCE\":importances, \n",
    "                                \"VARIABLE\":X_train.columns}).sort_values(\"IMPORTANCE\", ascending=False)\n",
    "dtf_importances['cumsum'] = dtf_importances['IMPORTANCE'].cumsum(axis=0)\n",
    "dtf_importances = dtf_importances.set_index(\"VARIABLE\")\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(25, 10))\n",
    "fig.suptitle(\"Features Importance\", fontsize=20)\n",
    "ax[0].title.set_text('variables')\n",
    "dtf_importances[[\"IMPORTANCE\"]].sort_values(by=\"IMPORTANCE\").plot(\n",
    "                kind=\"barh\", legend=False, ax=ax[0]).grid(axis=\"x\")\n",
    "ax[0].set(ylabel=\"\")\n",
    "ax[1].title.set_text('cumulative')\n",
    "dtf_importances[[\"cumsum\"]].plot(kind=\"line\", linewidth=4, \n",
    "                                 legend=False, ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", xticks=np.arange(len(dtf_importances)), \n",
    "          xticklabels=dtf_importances.index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo0BN4_tZmU4",
    "outputId": "3aec480c-9e8e-4b1c-f53f-c3ff21a13f07"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=('True vs Predicted',  'Residuals vs Predicted'))\n",
    "\n",
    "# Scatter plot True vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=np.array(y_test[\"Price\"]), \n",
    "                mode = 'markers', \n",
    "                name = 'True vs Predicted'\n",
    "                ),\n",
    "    row=1, \n",
    "    col=1\n",
    "            )\n",
    "\n",
    "# Scatter plot Residuals vs Predicted\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "                x=y_pred, \n",
    "                y=residuals, \n",
    "                mode = 'markers', \n",
    "                name = 'Residuals vs Predicted'\n",
    "              ),\n",
    "    row=1, \n",
    "    col=2\n",
    "            )\n",
    "\n",
    "# Setting theme, margin, and annotation in layout\n",
    "\n",
    "fig.update_layout(\n",
    "                    template=\"plotly_dark\", \n",
    "                    title=\"Gradient Boosting Regressor Best Results Plot\", \n",
    "                    xaxis_title=\"Predicted\", \n",
    "                    yaxis_title=\"True\",\n",
    "                    xaxis2_title=\"Predicted\", \n",
    "                    yaxis2_title=\"Residuals\",\n",
    "                    showlegend=False\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTajkpk-ZmU5"
   },
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(training_data= X_train.values, \n",
    "                                              feature_names= X_train.columns, \n",
    "                                              class_names = \"Price\",\n",
    "                                              mode=\"regression\")\n",
    "\n",
    "explained = explainer.explain_instance(X_test.values[2], new_gbr.predict, num_features=len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Urfc2o2JZmU6",
    "outputId": "2d339592-dad9-4546-e718-d432874ff543",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explained.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALQU0pbSZmU6",
    "outputId": "b6d2e819-818a-4c6b-a078-2255830fea7b"
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(new_gbr, X_train, y_train, n_repeats=30, random_state = 42, n_jobs=-1)\n",
    "perm_sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "x = result.importances[perm_sorted_idx].mean(axis=1).T\n",
    "y = X_train.columns[perm_sorted_idx]\n",
    "x_error = result.importances_std[perm_sorted_idx]\n",
    "sns.barplot(x=x, y=y, ax=ax, ci='sd')\n",
    "ax.errorbar(x, y, xerr = x_error, fmt='.', ecolor = 'black', color='black')\n",
    "ax.set_title(\"Permutation Importances (Train set)\", fontsize =16)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83hCt1Q1ZmU7",
    "outputId": "51652598-535e-41bd-f03c-a17d06268196"
   },
   "outputs": [],
   "source": [
    "pt = PrettyTable()\n",
    "pt.field_names = [\"Model\",\"Test R2 Scores\",\"Best Parameters\", \"Random State\"]\n",
    "pt.add_row(['Bagging Regressor', acc['Bagging Regressor'], best_param_dict['Bagging Regressor'], 42])\n",
    "pt.add_row(['Random Forest Regressor', acc['Random Forest Regressor'], best_param_dict['Random Forest Regressor'], 42])\n",
    "pt.add_row(['AdaBoost Regressor', acc['AdaBoost Regressor'], best_param_dict['AdaBoost Regressor'], 42])\n",
    "pt.add_row(['Gradient Boosting Regressor', acc['Gradient Boosting Regressor'], best_param_dict['Gradient Boosting Regressor'], 42])\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U05clE9LZmU8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Housing_Prices.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
